# -*- coding: utf-8 -*-
"""LiDA Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NOZ51YNksjTm7ErRGFxcWnErq2BsdPYc

<h1>Dependencies</h1>
"""

!pip install -U demoji sentence-transformers

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import word_tokenize
import re
from bs4 import BeautifulSoup
import demoji
from sentence_transformers import SentenceTransformer
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.layers import Bidirectional, Dense, LSTM
from tensorflow.keras.models import Sequential
from sklearn.metrics import classification_report

nltk.download('punkt')

"""<h1>Dataset</h1>"""

df = pd.read_csv("kr_train.tsv", sep = "\t")
df_test = pd.read_csv("kr_test.tsv", sep="\t")

df.head()

df_test.head()

df.describe()

df.shape

df.isnull().sum()

df["label"].value_counts()

num_words = df["tweet"].apply(lambda x: len(x.split()))
print("Max Words: ", max(num_words))
print("Min Words: ", min(num_words))

num_char = df["tweet"].apply(len)
print("Max Chars: ", max(num_char))
print("Min Chars: ", min(num_char))

def preprocess(data):
    data = data.lower()

    data = re.sub(r'@user','',data) #removing @user mentions in the tweet

    data = re.sub(r'[^a-zA-Z\s]', '', data) #removing symbols/punctuations

    #url management
    data = re.sub(r'http\S+', '', data)
    data = BeautifulSoup(data, 'lxml').get_text()

    data = demoji.replace(data,'') #remove emojis

    # tokens = word_tokenize(data) #tokenize

    return data

tweet = "@user performed augmentation. more info at https://www.abcd.com"
print(preprocess(tweet))

df["tweet"] = df["tweet"].apply(preprocess)
df.drop(['ID'],inplace=True,axis=1)

df_test["tweet"] = df_test["tweet"].apply(preprocess)
df_test.drop(['ID'],inplace=True,axis=1)

df.head()

# df.to_csv("preprocessed_train.csv")

label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label'])

df_test['label'] = label_encoder.fit_transform(df_test['label'])

"""<h1>Setup and Training</h1>"""

model = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')
embeddings = model.encode(df['tweet'])
test_embeddings = model.encode(df_test['tweet'])

print(embeddings.shape)
print(test_embeddings.shape)

"""<h3>Linear Embeddings</h3>"""

def linear_transformations(embedding):
  random_number = np.random.rand(*embedding.shape) * 0.01
  linear_embeddings = embedding + random_number
  return linear_embeddings

linear_embeddings = linear_transformations(embeddings)
linear_embeddings_test = linear_transformations(test_embeddings)

"""<h3>Autoencoder Embeddings</h3>"""

class AutoEncoder(tf.keras.Model):
    def __init__(self, embedding_dim, hidden_dim, learning_rate):
        super(AutoEncoder, self).__init__()
        self.encoder = tf.keras.Sequential([
            tf.keras.layers.Dense(hidden_dim, activation='relu', input_shape=(embedding_dim,))
        ])
        self.decoder = tf.keras.Sequential([
            tf.keras.layers.Dense(embedding_dim, activation='linear')
        ])
        self.optimizer = tf.keras.optimizers.Adam(learning_rate)
        self.loss_fn = tf.keras.losses.MeanSquaredError()

    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

    def train_step(self, x):
        with tf.GradientTape() as tape:
            reconstructions = self(x)
            loss = self.loss_fn(x, reconstructions)
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return {'loss': loss}

    def test_step(self, x):
        reconstructions = self(x)
        loss = self.loss_fn(x, reconstructions)
        return {'loss': loss}

embedding_dim = 768
hidden_dim = 300
learning_rate = 0.001
batch_size = 32

autoencoder = AutoEncoder(embedding_dim, hidden_dim, learning_rate)

dataset = tf.data.Dataset.from_tensor_slices(embeddings).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices(test_embeddings).batch(batch_size)

epochs = 10
for epoch in range(epochs):
    for batch in dataset:
        loss = autoencoder.train_step(batch)
    print(f'Train Epoch {epoch+1}, Loss: {loss["loss"]}')

    for batch in test_dataset:
        loss = autoencoder.train_step(batch)
    print(f'Test Epoch {epoch+1}, Loss: {loss["loss"]}')

ae_embeddings = []
ae_embeddings_test = []

for batch in dataset:
    reconstructions = autoencoder(batch)
    ae_embeddings.append(reconstructions.numpy())

ae_embeddings = np.concatenate(ae_embeddings, axis=0)

for batch in test_dataset:
    reconstructions = autoencoder(batch)
    ae_embeddings_test.append(reconstructions.numpy())

ae_embeddings_test = np.concatenate(ae_embeddings_test, axis=0)

"""<h3>Denoising Autoencoder Embeddings</h3>"""

class DenoisingAutoEncoder(tf.keras.Model):
    def __init__(self, embedding_dim, hidden_dim, learning_rate):
        super(DenoisingAutoEncoder, self).__init__()
        self.encoder = tf.keras.Sequential([
            tf.keras.layers.Dense(hidden_dim, activation='relu', input_shape=(embedding_dim,))
        ])
        self.decoder = tf.keras.Sequential([
            tf.keras.layers.Dense(embedding_dim, activation='linear')
        ])
        self.optimizer = tf.keras.optimizers.Adam(learning_rate)
        self.loss_fn = tf.keras.losses.MeanSquaredError()

    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

    def add_noise(self, x, noise_frac):
        noise = tf.random.normal(tf.shape(x), mean=0.0, stddev=0.1)
        noisy_x = x + noise_frac * noise
        return noisy_x

    def train_step(self, x, noise_frac=0.5):
        with tf.GradientTape() as tape:
            noisy_x = self.add_noise(x, noise_frac)
            reconstructions = self(noisy_x)
            loss = self.loss_fn(x, reconstructions)

        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return {'loss': loss}

    def test_step(self, x):

        reconstructions = self(x)
        loss = self.loss_fn(x, reconstructions)
        return {'loss': loss}

embedding_dim = 768
hidden_dim = 300
learning_rate = 0.001
batch_size = 32

denoising_autoencoder = DenoisingAutoEncoder(embedding_dim, hidden_dim, learning_rate)

dataset = tf.data.Dataset.from_tensor_slices(embeddings).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices(test_embeddings).batch(batch_size)

epochs = 10
noise_frac = 0.5

for epoch in range(epochs):
    for batch in dataset:
        loss = denoising_autoencoder.train_step(batch, noise_frac)
    print(f'Train Epoch {epoch+1}, Loss: {loss["loss"]}')
    for batch in test_dataset:
        loss = denoising_autoencoder.train_step(batch, noise_frac)
    print(f'Test Epoch {epoch+1}, Loss: {loss["loss"]}')


dae_embeddings = []
for batch in dataset:
    reconstructions = denoising_autoencoder(batch)
    dae_embeddings.append(reconstructions.numpy())

dae_embeddings = np.concatenate(dae_embeddings, axis=0)

dae_embeddings_test = []
for batch in test_dataset:
    reconstructions = denoising_autoencoder(batch)
    dae_embeddings_test.append(reconstructions.numpy())

dae_embeddings_test = np.concatenate(dae_embeddings_test, axis=0)

"""<h3>Concatenate Embeddings</h3>"""

final_embeddings = np.concatenate((linear_embeddings, ae_embeddings, dae_embeddings), axis=1)
final_embeddings_test = np.concatenate((linear_embeddings_test, ae_embeddings_test, dae_embeddings_test), axis=1)

"""<h1>BiLSTM Model for Classification and Metrics Evaluation</h1>"""

print(final_embeddings.shape)
print(final_embeddings_test.shape)

class CustomLSTM(tf.keras.Model):
    def __init__(self, hidden_dim, dropout_rate, learning_rate, num_classes):
        super(CustomLSTM, self).__init__()

        self.hidden_dim = hidden_dim
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.num_classes = num_classes

        self.lstm1 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)
        self.lstm2 = tf.keras.layers.LSTM(hidden_dim)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.lstm1(inputs)
        x = self.dropout(x)
        x = self.lstm2(x)
        x = self.dropout(x)
        x = self.fc(x)
        return x

hidden_dim = 500
dropout_rate = 0.5
learning_rate = 1e-5
num_classes = 3

model = CustomLSTM(hidden_dim, dropout_rate, learning_rate, num_classes)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

model.build(input_shape=(None, 22, 768))

model.summary()

model.fit(final_embeddings, df['label'], epochs=10, batch_size=32)