{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3417947838.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    data = pd.read_csv('C:\\Users\\Admin\\Desktop\\Data-Augmentation-DSSI\\Samar_LADA_implementation\\dataset\\kr_train.tsv', sep='\\t')\u001b[0m\n\u001b[1;37m                                                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Load the Kinyarwanda TSV dataset\n",
    "data = pd.read_csv('/content/kr_train.tsv', sep='\\t')\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data['tweet']\n",
    "y = data['label']\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=11)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lada_kinyarwanda_tsv(X_train, y_train, model, window_size=5, num_augmented_samples=10):\n",
    "    \"\"\"\n",
    "    Implement LADA approach for the Kinyarwanda TSV dataset.\n",
    "\n",
    "    Args:\n",
    "        X_train (csr_matrix): Training Kinyarwanda text data (TF-IDF vectorized).\n",
    "        y_train (pandas.Series): Training target.\n",
    "        model: The machine learning model to be used for LADA.\n",
    "        window_size (int): Size of the look-ahead window.\n",
    "        num_augmented_samples (int): Number of augmented samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        csr_matrix: Augmented training Kinyarwanda text data.\n",
    "        pandas.Series: Augmented training target.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store augmented data\n",
    "    X_augmented_data = []\n",
    "    X_augmented_indices = []\n",
    "    y_augmented = []  # Use a list to store augmented labels\n",
    "\n",
    "    for i in range(num_augmented_samples):\n",
    "        # Generate a random index within the training set\n",
    "        idx = np.random.randint(0, X_train.shape[0])\n",
    "\n",
    "        # Get the look-ahead window (adjusting for CSR matrix format)\n",
    "        start_idx = X_train.indptr[idx]\n",
    "        end_idx = X_train.indptr[idx + window_size] if idx + window_size < X_train.shape[0] else X_train.indptr[-1]\n",
    "\n",
    "        # Compute the uncertainty score for each sample in the look-ahead window\n",
    "        uncertainty_scores = [uncertainty_score(model, X_train[idx + j]) for j in range(window_size) if idx + j < X_train.shape[0]]\n",
    "\n",
    "        # Select the most informative sample(s) based on the uncertainty scores\n",
    "        most_informative_idx = np.argsort(uncertainty_scores)[-1]\n",
    "\n",
    "        # Extract the most informative sample using CSR matrix properties\n",
    "        most_informative_sample_data = X_train.data[start_idx + most_informative_idx : end_idx]\n",
    "        most_informative_sample_indices = X_train.indices[start_idx + most_informative_idx : end_idx]\n",
    "\n",
    "        # Append to augmented data\n",
    "        X_augmented_data.extend(most_informative_sample_data)\n",
    "        X_augmented_indices.extend(most_informative_sample_indices)\n",
    "        y_augmented.append(y_train.iloc[idx + most_informative_idx]) # Append the corresponding label to the list\n",
    "\n",
    "    # Calculate X_augmented_indptr directly from X_train.indptr\n",
    "    X_augmented_indptr = [0]\n",
    "    cumulative_count = 0\n",
    "    for i in range(len(y_augmented)):\n",
    "        cumulative_count += X_train.indptr[i+1] - X_train.indptr[i]\n",
    "        X_augmented_indptr.append(cumulative_count)\n",
    "\n",
    "    # Create a new CSR matrix from the augmented data\n",
    "    X_augmented = csr_matrix((X_augmented_data, X_augmented_indices, X_augmented_indptr), shape=(len(y_augmented), X_train.shape[1]))\n",
    "\n",
    "    # Convert the list of augmented labels to a Pandas Series\n",
    "    y_augmented = pd.Series(y_augmented)\n",
    "\n",
    "    return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Benchmark the performance of a given model.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to be evaluated.\n",
    "        X_train (csr_matrix): Training Kinyarwanda text data.\n",
    "        y_train (pandas.Series): Training target.\n",
    "        X_val (csr_matrix): Validation Kinyarwanda text data.\n",
    "        y_val (pandas.Series): Validation target.\n",
    "        X_test (csr_matrix): Test Kinyarwanda text data.\n",
    "        y_test (pandas.Series): Test target.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "     # Encode labels if necessary (for KMeans or other clustering models)\n",
    "    if hasattr(model, 'predict_proba'):  # Check if the model supports probability prediction\n",
    "        val_pred = model.predict(X_val)\n",
    "        test_pred = model.predict(X_test)\n",
    "    else:\n",
    "        # Handle models that don't have predict_proba (e.g., KMeans)\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(y_train)\n",
    "        y_val = label_encoder.transform(y_val)\n",
    "        y_test = label_encoder.transform(y_test)\n",
    "        val_pred = model.predict(X_val)\n",
    "        test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_f1': val_f1,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_score(model, X):\n",
    "    \"\"\"\n",
    "    Compute the entropy-based uncertainty score for a given sample.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to be used for uncertainty estimation.\n",
    "        X (csr_matrix): The input sample.\n",
    "\n",
    "    Returns:\n",
    "        float: The uncertainty score for the input sample.\n",
    "    \"\"\"\n",
    "    # Standardize the input sample (if applicable to your model)\n",
    "    # scaler = StandardScaler(with_mean=False)\n",
    "    # X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Compute the probability of each class\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_prob = model.predict_proba(X)[0]  # Assuming a single sample\n",
    "    else:\n",
    "        # Handle models without predict_proba (e.g., KMeans)\n",
    "        distances = model.transform(X)[0]\n",
    "        y_pred_prob = np.exp(-distances) / np.sum(np.exp(-distances))  # Softmax-like probabilities\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(y_pred_prob * np.log2(y_pred_prob + 1e-10))  # Add a small value to avoid log(0)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def uncertainty_score(model, X):\n",
    "    \"\"\"\n",
    "    Compute the uncertainty score for a given sample.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to be used for uncertainty estimation.\n",
    "        X (csr_matrix): The input sample.\n",
    "\n",
    "    Returns:\n",
    "        float: The uncertainty score for the input sample.\n",
    "    \"\"\"\n",
    "    # Standardize the input sample\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # For KMeans, calculate distance to cluster centers as a measure of uncertainty\n",
    "    distances = model.transform(X_scaled)[0]  # Calculate distances to all cluster centers\n",
    "    uncertainty_score = distances.min()  # Use the distance to the closest cluster\n",
    "\n",
    "    return uncertainty_score '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the model without data augmentation\n",
    "\n",
    "#model = LogisticRegression()\n",
    "model = KMeans(n_clusters = 5, random_state=11)\n",
    "baseline_metrics = benchmark_model(model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "print(\"Baseline metrics:\", baseline_metrics)\n",
    "\n",
    "# Benchmark the model with LADA data augmentation\n",
    "X_train_augmented, y_train_augmented = lada_kinyarwanda_tsv(X_train, y_train, model)\n",
    "#model = LogisticRegression()\n",
    "model = KMeans(n_clusters = 5, random_state=11)\n",
    "augmented_metrics = benchmark_model(model, X_train_augmented, y_train_augmented, X_val, y_val, X_test, y_test)\n",
    "print(\"Augmented metrics:\", augmented_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
