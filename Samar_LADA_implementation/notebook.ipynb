{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "text = [\"This is a sample text.\", \"Another sample text.\"]\n",
    "\n",
    "# creating embeddings\n",
    "sentence_embeddings = []\n",
    "for sentence in text:\n",
    "    inputs = tokenizer.encode_plus(sentence,\n",
    "                                  add_special_tokens=True,\n",
    "                                  max_length=512,\n",
    "                                  return_attention_mask=True,\n",
    "                                  return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    sentence_embeddings.append(torch.nn.functional.normalize(sentence_embedding, p=2, dim=1))\n",
    "\n",
    "\n",
    "\n",
    "sentence_embeddings = [embedding.detach().numpy() for embedding in sentence_embeddings]\n",
    "\n",
    "# Reshape sentence embeddings\n",
    "sentence_embeddings = [embedding.reshape(-1, embedding.shape[-1]) for embedding in sentence_embeddings]\n",
    "\n",
    "# Create LADA model\n",
    "kmeans = KMeans(n_clusters=2, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape sentence embeddings\n",
    "sentence_embeddings = [embedding.reshape(-1, embedding.shape[-1]) for embedding in sentence_embeddings]\n",
    "\n",
    "# Print the shape of embeddings\n",
    "for embedding in sentence_embeddings:\n",
    "  print(embedding.shape)\n",
    "\n",
    "\n",
    "#concatenate\n",
    "sentence_embeddings_array = np.concatenate(sentence_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cluster assignments for each data point\n",
    "print(\"Cluster assignments:\", kmeans.labels_)\n",
    "\n",
    "# Print indices of top points to add\n",
    "print(\"Top points to add:\", top_points)\n",
    "\n",
    "# Optionally, print the text of the selected data points\n",
    "for idx in top_points:\n",
    "    print(\"Selected text:\", text[idx])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
