{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = ['amh', 'hau', 'ibo', 'arq', 'ary', 'yor', 'por', 'twi', 'tso', 'tir', 'orm', 'pcm', 'kin', 'swa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at tir\n",
      "error at orm\n"
     ]
    }
   ],
   "source": [
    "ds_list = []\n",
    "\n",
    "for l in lang:\n",
    "    try:\n",
    "        ds = datasets.load_dataset('shmuhammad/AfriSenti-twitter-sentiment', l)\n",
    "        ds_list.append(ds)\n",
    "    except:\n",
    "        print(f'error at {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106828"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for ds in ds_list:\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        if split in ds:\n",
    "            count += ds[split].num_rows\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106828"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = []\n",
    "    \n",
    "for dataset in ds_list:\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        if split in dataset:\n",
    "            texts = dataset[split]['tweet']\n",
    "            full_text.extend(texts)\n",
    "\n",
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_dump.txt', 'w') as f:\n",
    "    for text in full_text:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text_dump.txt\n",
      "  input_format: \n",
      "  model_prefix: afrisenti\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 110000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <pad>\n",
      "  user_defined_symbols: <mask>\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  user_defined_symbols: <s>\n",
      "  user_defined_symbols: </s>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: text_dump.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 106828 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <mask>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=8998134\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=713\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 106828 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=4236013\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 384307 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 106828\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 318474\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 318474 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=179778 obj=14.5614 num_tokens=765212 num_tokens/piece=4.25643\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=160411 obj=12.9881 num_tokens=769563 num_tokens/piece=4.79745\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=120899 obj=13.0159 num_tokens=802011 num_tokens/piece=6.63373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=120639 obj=12.9706 num_tokens=802236 num_tokens/piece=6.64989\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: afrisenti.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: afrisenti.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='text_dump.txt',\n",
    "    model_prefix='afrisenti',\n",
    "    vocab_size=110000,\n",
    "    model_type='unigram',\n",
    "    character_coverage=0.9995,\n",
    "    user_defined_symbols=['<pad>', '<mask>','<sep>','<cls>', '<s>', '</s>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('afrisenti.model')\n",
    "\n",
    "def analyze_coverage(text):\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    unk_count = tokens.count('[UNK]')\n",
    "    return 1 - (unk_count / len(tokens))\n",
    "\n",
    "with open('text_dump.txt', 'r') as f:\n",
    "    texts = f.readlines()\n",
    "\n",
    "coverage = sum(analyze_coverage(text) for text in texts) / len(texts)\n",
    "print(f\"Token coverage: {coverage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece doesnt work with electra - kill me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c967b164bf466bb362613a9ca4f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 106828\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "def get_training_corpus(dataset, batch_size=1000):\n",
    "    for start_idx in range(0, len(dataset), batch_size):\n",
    "        samples = dataset[start_idx : start_idx + batch_size]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"text_dump.txt\"})[\"train\"]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with sentencepiece from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    get_training_corpus(dataset), \n",
    "    vocab_size=120000, \n",
    "    min_frequency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=120000, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_tokenizer = XLMRobertaTokenizerFast(\n",
    "    tokenizer_object=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xlmr-tokenizer/tokenizer_config.json',\n",
       " 'xlmr-tokenizer/special_tokens_map.json',\n",
       " 'xlmr-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('xlmr-tokenizer', exist_ok=True)\n",
    "xlmr_tokenizer.save_pretrained('xlmr-tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
